{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPool1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '/home/adam/R/Yelp/dataset/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/adam/R/Yelp/dataset/model_train.csv', usecols=['text', 'stars'])\n",
    "test = pd.read_csv('/home/adam/R/Yelp/dataset/model_test.csv', usecols = ['text', 'stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns = ['stars'])\n",
    "test = pd.get_dummies(test, columns = ['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(frac = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['text'].values\n",
    "y_train = train[['stars_1', 'stars_2', 'stars_3', 'stars_4', 'stars_5']]\n",
    "X_test = test['text'].values\n",
    "y_test = test[['stars_1', 'stars_2', 'stars_3', 'stars_4', 'stars_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 100\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval = 1):\n",
    "        super(Callback, self).__init__()\n",
    "        \n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose = 0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - socre: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPool1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(5, activation = 'sigmoid')(conc)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size = 0.95, random_state = 233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 133000 samples, validate on 7000 samples\n",
      "Epoch 1/2\n",
      " - 91s - loss: 0.3365 - acc: 0.8452 - val_loss: 0.2965 - val_acc: 0.8590\n",
      "\n",
      " ROC-AUC - epoch: 1 - socre: 0.884945 \n",
      "\n",
      "Epoch 2/2\n",
      " - 90s - loss: 0.2870 - acc: 0.8647 - val_loss: 0.2836 - val_acc: 0.8652\n",
      "\n",
      " ROC-AUC - epoch: 2 - socre: 0.894973 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                callbacks = [RocAuc], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000/70000 [==============================] - 8s 108us/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([x_test], batch_size=1024, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(y_test, columns = [['stars_1', 'stars_2', 'stars_3', 'stars_4', 'stars_5']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.to_csv('gru_fasttext_preds.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>stars_1</th>\n",
       "      <th>stars_2</th>\n",
       "      <th>stars_3</th>\n",
       "      <th>stars_4</th>\n",
       "      <th>stars_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.592002</td>\n",
       "      <td>0.356129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>0.122033</td>\n",
       "      <td>0.887182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>0.117154</td>\n",
       "      <td>0.885646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.123683</td>\n",
       "      <td>0.377102</td>\n",
       "      <td>0.308473</td>\n",
       "      <td>0.047827</td>\n",
       "      <td>0.011795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.011115</td>\n",
       "      <td>0.122977</td>\n",
       "      <td>0.886352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.307285</td>\n",
       "      <td>0.389155</td>\n",
       "      <td>0.253022</td>\n",
       "      <td>0.032887</td>\n",
       "      <td>0.009971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>0.106488</td>\n",
       "      <td>0.446022</td>\n",
       "      <td>0.554152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.213599</td>\n",
       "      <td>0.839225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.090557</td>\n",
       "      <td>0.924011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.143498</td>\n",
       "      <td>0.594986</td>\n",
       "      <td>0.377356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.160516</td>\n",
       "      <td>0.669866</td>\n",
       "      <td>0.137441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.051043</td>\n",
       "      <td>0.958891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.454549</td>\n",
       "      <td>0.288364</td>\n",
       "      <td>0.144445</td>\n",
       "      <td>0.023721</td>\n",
       "      <td>0.027754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>0.976576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.034389</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.106156</td>\n",
       "      <td>0.346877</td>\n",
       "      <td>0.484345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.055083</td>\n",
       "      <td>0.703166</td>\n",
       "      <td>0.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>0.258948</td>\n",
       "      <td>0.752400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.749263</td>\n",
       "      <td>0.184167</td>\n",
       "      <td>0.073911</td>\n",
       "      <td>0.030430</td>\n",
       "      <td>0.008257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.042577</td>\n",
       "      <td>0.421909</td>\n",
       "      <td>0.621298</td>\n",
       "      <td>0.079835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.192296</td>\n",
       "      <td>0.728306</td>\n",
       "      <td>0.087655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.007205</td>\n",
       "      <td>0.183058</td>\n",
       "      <td>0.810911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.025501</td>\n",
       "      <td>0.718581</td>\n",
       "      <td>0.341573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>0.231305</td>\n",
       "      <td>0.765574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.090411</td>\n",
       "      <td>0.724454</td>\n",
       "      <td>0.235422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.025296</td>\n",
       "      <td>0.383826</td>\n",
       "      <td>0.639675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.759050</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.007775</td>\n",
       "      <td>0.002530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.089611</td>\n",
       "      <td>0.248091</td>\n",
       "      <td>0.376060</td>\n",
       "      <td>0.128983</td>\n",
       "      <td>0.073537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.879134</td>\n",
       "      <td>0.062447</td>\n",
       "      <td>0.007416</td>\n",
       "      <td>0.007755</td>\n",
       "      <td>0.007163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.023332</td>\n",
       "      <td>0.277449</td>\n",
       "      <td>0.522432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.108862</td>\n",
       "      <td>0.885250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69970</th>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.034544</td>\n",
       "      <td>0.963869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69971</th>\n",
       "      <td>0.941561</td>\n",
       "      <td>0.028534</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.040333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69972</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.166258</td>\n",
       "      <td>0.780900</td>\n",
       "      <td>0.093362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69973</th>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.056292</td>\n",
       "      <td>0.944411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69974</th>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.072757</td>\n",
       "      <td>0.733407</td>\n",
       "      <td>0.290557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69975</th>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.142567</td>\n",
       "      <td>0.654310</td>\n",
       "      <td>0.179799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69976</th>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.144356</td>\n",
       "      <td>0.860701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69977</th>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.123103</td>\n",
       "      <td>0.860394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69978</th>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>0.530597</td>\n",
       "      <td>0.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69979</th>\n",
       "      <td>0.024591</td>\n",
       "      <td>0.422778</td>\n",
       "      <td>0.641838</td>\n",
       "      <td>0.033813</td>\n",
       "      <td>0.002122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69980</th>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.216915</td>\n",
       "      <td>0.763084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69981</th>\n",
       "      <td>0.062574</td>\n",
       "      <td>0.500109</td>\n",
       "      <td>0.442280</td>\n",
       "      <td>0.044969</td>\n",
       "      <td>0.008127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69982</th>\n",
       "      <td>0.073720</td>\n",
       "      <td>0.199172</td>\n",
       "      <td>0.477433</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.027498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69983</th>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.013360</td>\n",
       "      <td>0.331750</td>\n",
       "      <td>0.579481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69984</th>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.015245</td>\n",
       "      <td>0.232753</td>\n",
       "      <td>0.817736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69985</th>\n",
       "      <td>0.007303</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.054046</td>\n",
       "      <td>0.246059</td>\n",
       "      <td>0.823188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69986</th>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.119486</td>\n",
       "      <td>0.849211</td>\n",
       "      <td>0.088503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69987</th>\n",
       "      <td>0.031761</td>\n",
       "      <td>0.067776</td>\n",
       "      <td>0.410075</td>\n",
       "      <td>0.306162</td>\n",
       "      <td>0.160737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69988</th>\n",
       "      <td>0.065303</td>\n",
       "      <td>0.323776</td>\n",
       "      <td>0.591346</td>\n",
       "      <td>0.140513</td>\n",
       "      <td>0.022257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69989</th>\n",
       "      <td>0.029116</td>\n",
       "      <td>0.256064</td>\n",
       "      <td>0.432202</td>\n",
       "      <td>0.212932</td>\n",
       "      <td>0.027677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69990</th>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>0.431362</td>\n",
       "      <td>0.619384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69991</th>\n",
       "      <td>0.728965</td>\n",
       "      <td>0.252771</td>\n",
       "      <td>0.068016</td>\n",
       "      <td>0.023721</td>\n",
       "      <td>0.014140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69992</th>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.057852</td>\n",
       "      <td>0.380916</td>\n",
       "      <td>0.599436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69993</th>\n",
       "      <td>0.354635</td>\n",
       "      <td>0.222573</td>\n",
       "      <td>0.305451</td>\n",
       "      <td>0.146364</td>\n",
       "      <td>0.047622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69994</th>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.003412</td>\n",
       "      <td>0.111135</td>\n",
       "      <td>0.812129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.038295</td>\n",
       "      <td>0.139326</td>\n",
       "      <td>0.394698</td>\n",
       "      <td>0.382247</td>\n",
       "      <td>0.059683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0.752465</td>\n",
       "      <td>0.260030</td>\n",
       "      <td>0.023908</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.006501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>0.270064</td>\n",
       "      <td>0.738080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.785497</td>\n",
       "      <td>0.069562</td>\n",
       "      <td>0.030451</td>\n",
       "      <td>0.024802</td>\n",
       "      <td>0.014058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0.191267</td>\n",
       "      <td>0.105776</td>\n",
       "      <td>0.215408</td>\n",
       "      <td>0.399109</td>\n",
       "      <td>0.109915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stars_1   stars_2   stars_3   stars_4   stars_5\n",
       "0      0.001430  0.002741  0.057203  0.592002  0.356129\n",
       "1      0.001737  0.002654  0.010059  0.122033  0.887182\n",
       "2      0.000700  0.000317  0.003364  0.117154  0.885646\n",
       "3      0.123683  0.377102  0.308473  0.047827  0.011795\n",
       "4      0.001725  0.001485  0.011115  0.122977  0.886352\n",
       "5      0.307285  0.389155  0.253022  0.032887  0.009971\n",
       "6      0.004798  0.012381  0.106488  0.446022  0.554152\n",
       "7      0.001388  0.000744  0.005315  0.213599  0.839225\n",
       "8      0.000298  0.000165  0.001636  0.090557  0.924011\n",
       "9      0.001129  0.007314  0.143498  0.594986  0.377356\n",
       "10     0.000972  0.010928  0.160516  0.669866  0.137441\n",
       "11     0.000545  0.000121  0.000836  0.051043  0.958891\n",
       "12     0.454549  0.288364  0.144445  0.023721  0.027754\n",
       "13     0.000800  0.000146  0.000677  0.022198  0.976576\n",
       "14     0.034389  0.016484  0.106156  0.346877  0.484345\n",
       "15     0.000084  0.000966  0.055083  0.703166  0.336900\n",
       "16     0.000878  0.000462  0.004710  0.258948  0.752400\n",
       "17     0.749263  0.184167  0.073911  0.030430  0.008257\n",
       "18     0.002431  0.042577  0.421909  0.621298  0.079835\n",
       "19     0.002839  0.010491  0.192296  0.728306  0.087655\n",
       "20     0.000258  0.000354  0.007205  0.183058  0.810911\n",
       "21     0.000343  0.000615  0.025501  0.718581  0.341573\n",
       "22     0.000101  0.000286  0.005548  0.231305  0.765574\n",
       "23     0.000327  0.004060  0.090411  0.724454  0.235422\n",
       "24     0.000512  0.001478  0.025296  0.383826  0.639675\n",
       "25     0.759050  0.102190  0.021862  0.007775  0.002530\n",
       "26     0.089611  0.248091  0.376060  0.128983  0.073537\n",
       "27     0.879134  0.062447  0.007416  0.007755  0.007163\n",
       "28     0.001343  0.002038  0.023332  0.277449  0.522432\n",
       "29     0.000446  0.000163  0.002037  0.108862  0.885250\n",
       "...         ...       ...       ...       ...       ...\n",
       "69970  0.001941  0.000221  0.001020  0.034544  0.963869\n",
       "69971  0.941561  0.028534  0.003088  0.002570  0.040333\n",
       "69972  0.000100  0.002397  0.166258  0.780900  0.093362\n",
       "69973  0.001222  0.000157  0.001416  0.056292  0.944411\n",
       "69974  0.000550  0.002400  0.072757  0.733407  0.290557\n",
       "69975  0.000204  0.002408  0.142567  0.654310  0.179799\n",
       "69976  0.001166  0.000522  0.006716  0.144356  0.860701\n",
       "69977  0.000708  0.000237  0.002004  0.123103  0.860394\n",
       "69978  0.000326  0.000530  0.019118  0.530597  0.384900\n",
       "69979  0.024591  0.422778  0.641838  0.033813  0.002122\n",
       "69980  0.000651  0.000833  0.009474  0.216915  0.763084\n",
       "69981  0.062574  0.500109  0.442280  0.044969  0.008127\n",
       "69982  0.073720  0.199172  0.477433  0.216200  0.027498\n",
       "69983  0.000296  0.000499  0.013360  0.331750  0.579481\n",
       "69984  0.001086  0.001639  0.015245  0.232753  0.817736\n",
       "69985  0.007303  0.006189  0.054046  0.246059  0.823188\n",
       "69986  0.000814  0.004027  0.119486  0.849211  0.088503\n",
       "69987  0.031761  0.067776  0.410075  0.306162  0.160737\n",
       "69988  0.065303  0.323776  0.591346  0.140513  0.022257\n",
       "69989  0.029116  0.256064  0.432202  0.212932  0.027677\n",
       "69990  0.001006  0.000517  0.009025  0.431362  0.619384\n",
       "69991  0.728965  0.252771  0.068016  0.023721  0.014140\n",
       "69992  0.001265  0.002521  0.057852  0.380916  0.599436\n",
       "69993  0.354635  0.222573  0.305451  0.146364  0.047622\n",
       "69994  0.002905  0.000567  0.003412  0.111135  0.812129\n",
       "69995  0.038295  0.139326  0.394698  0.382247  0.059683\n",
       "69996  0.752465  0.260030  0.023908  0.007129  0.006501\n",
       "69997  0.000516  0.000578  0.008140  0.270064  0.738080\n",
       "69998  0.785497  0.069562  0.030451  0.024802  0.014058\n",
       "69999  0.191267  0.105776  0.215408  0.399109  0.109915\n",
       "\n",
       "[70000 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000/70000 [==============================] - 8s 108us/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2816612017086574, 0.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     3000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 100, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 160)     182880      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 160)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 160)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 320)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            1605        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,184,485\n",
      "Trainable params: 3,184,485\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model2():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPool1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(5, activation = 'sigmoid')(conc)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = get_model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 133000 samples, validate on 7000 samples\n",
      "Epoch 1/2\n",
      " - 78s - loss: 0.3518 - acc: 0.8400 - val_loss: 0.3168 - val_acc: 0.8519\n",
      "\n",
      " ROC-AUC - epoch: 1 - socre: 0.865366 \n",
      "\n",
      "Epoch 2/2\n",
      " - 77s - loss: 0.3095 - acc: 0.8548 - val_loss: 0.3006 - val_acc: 0.8576\n",
      "\n",
      " ROC-AUC - epoch: 2 - socre: 0.880859 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist2 = model2.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                callbacks = [RocAuc], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000/70000 [==============================] - 8s 111us/step\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(x_test, y_test, verbose=1, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.299527583530971, 0.8585457019124713]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model3():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(GRU(128, dropout = 0.3, recurrent_dropout=0.5, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPool1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(5, activation = 'sigmoid')(conc)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = get_model3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 133000 samples, validate on 7000 samples\n",
      "Epoch 1/2\n",
      " - 89s - loss: 0.3695 - acc: 0.8343 - val_loss: 0.3281 - val_acc: 0.8499\n",
      "\n",
      " ROC-AUC - epoch: 1 - socre: 0.856875 \n",
      "\n",
      "Epoch 2/2\n",
      " - 89s - loss: 0.3251 - acc: 0.8496 - val_loss: 0.3101 - val_acc: 0.8545\n",
      "\n",
      " ROC-AUC - epoch: 2 - socre: 0.873605 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist3 = model3.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                callbacks = [RocAuc], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
